"""
Text processing and chunking operations
"""

from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

from config.settings import settings
from ingestion.vector_store import add_documents_to_vector_store

class TextProcessor:
    """Text processing and chunking class"""
    
    def __init__(self):
        """Initialize text splitter"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def process_documents(self, documents: List[Document]) -> List[Document]:
        """
        Process documents and split into chunks
        
        Args:
            documents: Documents to process
            
        Returns:
            Processed and chunked documents
        """
        if not documents:
            return []
        
        print(f"ğŸ“ {len(documents)} belge iÅŸleniyor...")
        
        all_chunks = []
        
        for doc in documents:
            # Belgeyi chunk'lara bÃ¶l
            chunks = self.text_splitter.split_documents([doc])
            
            # Her chunk iÃ§in metadata gÃ¼ncelle
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'chunk_size': len(chunk.page_content)
                })
            
            all_chunks.extend(chunks)
            
            print(f"ğŸ“„ Ä°ÅŸleniyor: {doc.metadata.get('source', 'unknown')}")
            print(f"âœ‚ï¸  {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼")
        
        print(f"ğŸ‰ Toplam {len(all_chunks)} metin parÃ§asÄ± oluÅŸturuldu")
        return all_chunks
    
    def process_and_store_documents(self, documents: List[Document]) -> bool:
        """
        Belgeleri iÅŸle ve vector store'a ekle
        
        Args:
            documents: Ä°ÅŸlenecek belgeler
            
        Returns:
            bool: BaÅŸarÄ±lÄ± ise True
        """
        try:
            # Belgeleri iÅŸle
            processed_docs = self.process_documents(documents)
            
            if not processed_docs:
                print("âš ï¸  Ä°ÅŸlenecek belge bulunamadÄ±")
                return False
            
            # Vector store'a ekle
            success = add_documents_to_vector_store(processed_docs)
            
            return success
            
        except Exception as e:
            print(f"âŒ Belge iÅŸleme ve saklama hatasÄ±: {e}")
            return False
    
    def _clean_text(self, text: str) -> str:
        """
        Metni temizler
        
        Args:
            text: Temizlenecek metin
            
        Returns:
            TemizlenmiÅŸ metin
        """
        if not text:
            return ""
        
        # Fazla boÅŸluklarÄ± ve gereksiz karakterleri temizle
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # SatÄ±r baÅŸÄ± ve sonundaki boÅŸluklarÄ± temizle
            line = line.strip()
            
            # BoÅŸ satÄ±rlarÄ± ve Ã§ok kÄ±sa satÄ±rlarÄ± atla
            if len(line) > 2:
                # Fazla boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
                line = ' '.join(line.split())
                cleaned_lines.append(line)
        
        # SatÄ±rlarÄ± birleÅŸtir
        cleaned_text = '\n'.join(cleaned_lines)
        
        # Fazla satÄ±r sonlarÄ±nÄ± temizle
        while '\n\n\n' in cleaned_text:
            cleaned_text = cleaned_text.replace('\n\n\n', '\n\n')
        
        return cleaned_text.strip()
    
    def get_chunk_statistics(self, chunks: List[Document]) -> dict:
        """
        Chunk istatistiklerini dÃ¶ndÃ¼rÃ¼r
        
        Args:
            chunks: Document chunk'larÄ±
            
        Returns:
            Ä°statistik bilgileri
        """
        if not chunks:
            return {"total_chunks": 0}
        
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "average_chunk_size": sum(chunk_sizes) / len(chunks),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes),
            "total_characters": sum(chunk_sizes),
            "documents_processed": len(set(chunk.metadata.get("filename", "") for chunk in chunks))
        } 